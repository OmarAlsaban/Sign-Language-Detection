{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afac4a7e-f8df-48c5-81bb-5ccd1172f609",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**1-Import and Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de812153-0bff-479b-902e-4919a185652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbe167c-cc76-42dc-b68c-856701bcfd60",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891b09b4-defa-45e9-abbd-b95d1c19c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp \n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing         #Drawing utilities\n",
    "from mediapipe.python.solutions import holistic as mp_holistic             #Holistic model\n",
    "from mediapipe.python.solutions import hands as mp_hands                   # Hand info\n",
    "from mediapipe.python.solutions import face_mesh  as mp_face_mesh          # Face info\n",
    "from mediapipe.python.solutions import pose as mp_pose                     # Pose info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d96795-8cdb-4d65-9ba7-b78dc7f4f8c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**2-Keypoints using MP Holistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8860a446-5f5f-4572-806d-3e787058d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mp_holistic = mp.solutions.holistic #Holistic model\n",
    "#mp_drawing = mp.solutions.download_utils #Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52c74b0-8e1e-44b6-ba9b-7f5ff651e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image =cv2.cvtColor(image,cv2.COLOR_BGR2RGB) # Color Convertion BGR 2 RGB\n",
    "    image.flags.writeable = False                # image is no longer writeable\n",
    "    results = model.process(image)               # Make Prediction\n",
    "    image.flags.writeable = True                 # image is writeable\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR) # Color Convertion RGB 2 BGR\n",
    "    return image , results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa488b6-d962-4bea-ab69-6a2168f0b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawing_the_landmarks(image , results):\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image , results.face_landmarks , mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(0,70,255),thickness=1,circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(0,255,0),thickness=1,circle_radius=1))      # Drawing the FACE CONNECTIONS\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image , results.pose_landmarks,mp_pose.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(0,70,255),thickness=1,circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(0,255,0),thickness=1,circle_radius=1))      # Drawing the POSE CONNECTIONS\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image , results.right_hand_landmarks,mp_hands.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(0,70,255),thickness=1,circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(0,255,0),thickness=1,circle_radius=1))       # Drawing the RIGHT HAND CONNECTIONS\n",
    "\n",
    "    mp_drawing.draw_landmarks(image , results.left_hand_landmarks,mp_hands.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(0,70,255),thickness=1,circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(0,255,0),thickness=1,circle_radius=1))        # Drawing the LEFT HAND CONNECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fef787f-6cdf-4c74-8483-f5e6bda1d1b8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)                             # Open our Webcam\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic :  # Set mediapipe model\n",
    "    while cap.isOpened():                                  # as the cam is still open\n",
    "        ret, frame = cap.read()                            # Read the cam feed\n",
    "        frame = cv2.flip(frame,1)\n",
    "        image , results = mediapipe_detection(frame,holistic) # Make detection\n",
    "        #print(results)\n",
    "\n",
    "        drawing_the_landmarks(image , results)\n",
    "        cv2.imshow('OpenCV Feed',image)                    # Show feed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):             #press 'q' to end feed\n",
    "            break\n",
    "    cap.release()                                          #End Cap\n",
    "    cv2.destroyAllWindows()                                #Close cam Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb3f7b-feaa-4947-96f9-6bb5d16e3cf5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**3-Extract Keypoint Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30bba2cc-226a-4733-8a85-34aeb38ee9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Extract_Keypoints(results):\n",
    "    \n",
    "    #Get the Face Landmarks if exist, and zeros if not \n",
    "    if results.face_landmarks:\n",
    "        Face_marks= np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        Face_marks= np.zeros(33*4) \n",
    "\n",
    "    #Get the Pose Landmarks if exist, and zeros if not\n",
    "    if results.pose_landmarks:\n",
    "        Pose_marks= np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten()  \n",
    "    else:\n",
    "        Pose_marks= np.zeros(468*3)\n",
    "    \n",
    "    #Get the R Hand Landmarks if exist, and zeros if not\n",
    "    if results.right_hand_landmarks:\n",
    "        R_Hand_marks = np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten()  \n",
    "    else:\n",
    "        R_Hand_marks = np.zeros(21*3)\n",
    "    #Get the L Hand Landmarks if exist, and zeros if not\n",
    "    if results.left_hand_landmarks:\n",
    "        L_Hand_marks= np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() \n",
    "    else:\n",
    "        L_Hand_marks = np.zeros(21*3)\n",
    "    return np.concatenate([Pose_marks,Face_marks,L_Hand_marks,R_Hand_marks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ae333-0454-4d70-84ad-dde7809c624b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**4-Setup Folders for Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6fee485-73f4-466a-b92d-7defc5c0417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('MP_data')                  # Path for exported Numpy arrays\n",
    "actions = np.array(['Hello','Good','Excellent'])   # Actions that we try to detect\n",
    "no_sequences=30                                   # Thirty videos worth of data\n",
    "sequences_lenght=30                                 # 30 frames for the Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d1bdcd6-710a-47ef-a648-75c73f50add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for action in actions:\n",
    "#     for sequence in range(no_sequences):\n",
    "#         try:\n",
    "#             os.makedirs(os.path.join(DATA_PATH,action,str(sequence)))\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d3ee7-bb19-43bf-8c71-e9d86bafa044",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**5-Collect Keypoint Values for Training and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "540bb4ec-b6c1-44fc-8d8f-72c703849f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)                             # Open our Webcam\n",
    "\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic :  # Set mediapipe model\n",
    "    \n",
    "#     for action in actions:                                         # for the Action named ...\n",
    "#         for sequence in range(no_sequences):                       # for the sequence number ...\n",
    "#             for frame_num in range(sequences_lenght):              # for the frame number ...\n",
    "                \n",
    "        \n",
    "#                 ret, frame = cap.read()                            # Read the cam feed\n",
    "#                 frame = cv2.flip(frame,1)                          # Flip the cam feed on the y axis\n",
    "        \n",
    "#                 image , results = mediapipe_detection(frame,holistic) # Make detection of the pose, face, and hands coordinations\n",
    "        \n",
    "#                 drawing_the_landmarks(image , results)                # Draw the marks of the detection\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "#                 #Wait for sometime at the start of every new sequence\n",
    "                \n",
    "#                 if frame_num == 0:\n",
    "#                     cv2.putText(image,'Starting Collection', (120,200),\n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0),4,cv2.LINE_AA)\n",
    "#                     cv2.putText(image,'Collecting Frames for {} Video Number {}'.format(action,sequence), (15,12),\n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255),1,cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Feed',image)                    # Show feed\n",
    "#                     cv2.waitKey(2000)\n",
    "#                 else:\n",
    "                    \n",
    "#                     cv2.putText(image,'Collecting Frames for {} Video Number {}'.format(action,sequence), (15,12),\n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255),1,cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Feed',image)                    # Show feed\n",
    "\n",
    "#                 #Extract and Save the landmarks as numpy arrays\n",
    "#                 Key_Points_Export = Extract_Keypoints(results)\n",
    "#                 npy_Path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "#                 np.save(npy_Path, Key_Points_Export)\n",
    "\n",
    "                \n",
    "                \n",
    "        \n",
    "#                 if cv2.waitKey(10) & 0xFF == ord('q'):             #press 'q' to end feed\n",
    "#                     break\n",
    "            \n",
    "#     cap.release()                                          #End Cap\n",
    "#     cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd85e5-b069-4ce8-ae4b-ff1ab48d3136",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**6-Preprocess Data and Create Labels and Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30dba588-d2ae-4c9f-971c-ab9dc80038fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3860f4bf-c826-4796-b769-145bfcd1222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_map = {label:num for num,label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "308e18a0-3fd2-47cc-84c1-689be9158fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 0, 'Good': 1, 'Excellent': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e83dd49-0c28-4d34-b6a4-83c28eaadfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences , Label = [] , []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        win = []\n",
    "        for frame_num in range(sequences_lenght):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), '{}.npy'.format(frame_num)))\n",
    "            win.append(res)\n",
    "        sequences.append(win)\n",
    "        Label.append(Label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2b0fb5b-9c66-40b3-8718-462239373ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 30, 1662)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6c8b9f4-6cbf-4fea-9256-bb3f60f33dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Label).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23269630-3f65-4412-8364-afd675a20c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 30, 1662)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(sequences)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90522b31-085c-4882-a11d-684aa103f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(Label).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daac1649-c0c0-4e97-bb94-2b766d58a22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c3800a3-aaca-43cd-8697-f9833d5139c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a28ae296-38c5-4eb6-9fcb-adb4b1d808b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98355e3b-63d3-48ef-810c-1d85b50df753",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**7-Build and Train LSTM Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a523d8aa-672a-4b3e-b76d-17b65bfbda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM , Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57bbd60b-76f3-4393-9d91-f9e2f63a848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir = logging_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c446b7e-3a3e-46c2-94c8-434626ece969",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f55d585-8236-4ad9-9b00-407b73060cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "base_lr = 0.005\n",
    "# including a global_clipnorm is extremely important in object detection tasks\n",
    "optimizer = keras.optimizers.SGD(\n",
    "    learning_rate=base_lr, momentum=0.85, global_clipnorm=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e8467a3-2fdf-466d-8a01-0ceaf85316f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff87329-4ef4-4f70-b0d3-fa5dd66df019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,epochs=300,callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c533e-3602-44a3-bd1a-7ce5922eb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79c6db-f6ef-4ef7-ad7b-8b79fa5dfc01",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**8-Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c4e6c-e758-4176-9f53-cb4ebde3a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mod = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081051b9-655d-4373-a2f2-ed4b9a1b45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res_mod[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97560583-2f48-4cfc-9021-ad36497998b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bc6f4-1020-4fcc-a74e-3be3dc7c33fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**9-Save Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc631b-8e4b-44dd-8d77-ac0265c27c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67df5b46-66a8-45b7-bb22-62f930e07d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c30a28-3be1-445e-8cfd-9da93f1076db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**10-Evaluation using Confusion Matrix and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77d6086f-c2b0-4e06-b339-f5ff4d0c3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f895fdb-a9ca-4df9-9ce4-c8b46a4a838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 658ms/step\n"
     ]
    }
   ],
   "source": [
    "ytry = model.predict(x_test)\n",
    "ytrue = np.argmax(y_test, axis= 1 ).tolist()\n",
    "ytry = np.argmax(ytry, axis= 1 ).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "000feebe-85fc-48d9-9fad-bb2c759560ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3, 0],\n",
       "        [0, 2]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [0, 2]],\n",
       "\n",
       "       [[4, 0],\n",
       "        [0, 1]]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue,ytry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f92a3fc3-fec8-4a34-a9f8-cd1c93a3b025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue,ytry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387c5f0-77e1-4ca0-a7d3-9f520c80c289",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**11-Test in Real Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff31a008-7ca7-41e6-8b12-9256a4eb3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16),(117,245,16),(16,117,145)]\n",
    "def prob_viz(res,actions,input_frame,colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num , prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame,(0, 60 + num*40),(int(prob*100), 90 + num*40), colors[num],-1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85 + num*40),cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8178b5a7-6869-4ba5-b2f7-95df8826a2ef",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "Sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.6        # Detection Variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)                             # Open our Webcam\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic :  # Set mediapipe model\n",
    "    while cap.isOpened():                                  # as the cam is still open\n",
    "        ret, frame = cap.read()                            # Read the cam feed\n",
    "        frame = cv2.flip(frame,1)\n",
    "        image , results = mediapipe_detection(frame,holistic) # Make detection\n",
    "        #print(results)\n",
    "\n",
    "\n",
    "        keypoints = Extract_Keypoints(results)\n",
    "        Sequence.append(keypoints)\n",
    "        Sequence = Sequence[-30:]\n",
    "        if len(Sequence) == 30:\n",
    "            Res = model.predict(np.expand_dims(Sequence, axis=0))[0]\n",
    "            #print(actions[np.argmax(Res)])\n",
    "            predictions.append(np.argmax(Res))\n",
    "\n",
    "\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(Res):\n",
    "                if Res[np.argmax(Res)] > threshold:\n",
    "                    \n",
    "                    if len(sentence) > 0:\n",
    "                        if actions[np.argmax(Res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(Res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(Res)])\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            image = prob_viz(Res,actions,image,colors)\n",
    "        cv2.rectangle(image, (0,0) , (640,40), (245,117,16), -1 )\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "                \n",
    "        \n",
    "        cv2.imshow('OpenCV Feed',image)                    # Show feed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):             #press 'q' to end feed\n",
    "            break\n",
    "    cap.release()                                          #End Cap\n",
    "    cv2.destroyAllWindows()                                #Close cam Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76b361-bd39-4f24-8495-a6df03d225be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()                                          #End Cap\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900badcc-2ab1-4706-93b2-464732e49aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a44f0b-91e2-4ce6-8ca5-a77006b54943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
